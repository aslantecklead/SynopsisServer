WEBVTT

00:00.090 --> 00:02.910
When you make as many videos as we do, you need a lot of fast,

00:02.980 --> 00:06.474
reliable storage. And our main editing server,

00:06.522 --> 00:09.726
Wanick, has checked all of those boxes for years.

00:09.828 --> 00:13.226
It's a great little server. It's built out of high quality components

00:13.258 --> 00:16.938
and it even looks cool. But as our team has grown, we've reached

00:16.954 --> 00:20.586
the point where even a minute, one single minute

00:20.618 --> 00:24.982
of downtime costs over $50. And that's just

00:25.076 --> 00:28.626
in payroll. Now, practically speaking, the way to mitigate

00:28.658 --> 00:32.390
that is by adding redundancy. Now, our drives are already

00:32.460 --> 00:36.326
redundant. We've got 20 drives in there with data striping. But the problem

00:36.428 --> 00:39.918
is they all sit in one single server.

00:40.034 --> 00:43.626
And I'm sure you can see where this is going. It's been over a year

00:43.648 --> 00:47.306
in the making, but it's finally here. Wanick final form.

00:47.408 --> 00:50.538
And I'm calling it Wanik ten,

00:50.704 --> 00:52.860
because it's the last wanik ever.

00:54.350 --> 00:58.138
Just like ten times. Nobody even knows what high availability

00:58.234 --> 01:01.726
means. It means it's Linus proof. Just go ahead, unplug one. Do it.

01:01.748 --> 01:04.842
Go for it. Well, okay. I should probably tell you the stakes

01:04.906 --> 01:08.290
before you do that. Each of these two grand twin boxes has four

01:08.360 --> 01:12.418
entire servers inside of them that were provided by Supermicro, who sponsored this whole thing.

01:12.504 --> 01:16.158
And they're set up with WECA, a redundant Nvme first file

01:16.174 --> 01:20.354
system. In this config, it should sustain two entire servers

01:20.402 --> 01:23.526
dropping out without anyone even noticing. Except that we

01:23.548 --> 01:27.106
moved the entire team onto it last night without telling anyone. And it's

01:27.138 --> 01:30.150
the middle of the workday with a ton of high priority videos in progress.

01:30.230 --> 01:33.914
Do you really want to test it right now? I haven't tried that.

01:33.952 --> 01:37.340
All right, here we go. What could go wrong?

01:38.430 --> 01:39.660
I mean, a lot.

01:48.850 --> 01:52.426
Naturally, a huge part of a project like this is the software.

01:52.538 --> 01:55.886
The stuff that's going to handle distributing all of our hundred ish

01:55.918 --> 02:00.062
terabytes of video projects, word documents, and Linux

02:00.126 --> 02:03.346
ISOs to the multiple machines that we just showed you.

02:03.448 --> 02:06.566
But we can't install any software until we have some

02:06.588 --> 02:10.754
hardware. So why don't we start there? Meet the Supermicro

02:10.802 --> 02:14.434
grand twin. A plus server as 2115 gt

02:14.562 --> 02:18.274
Hntr. Despite its sort of ordinary

02:18.322 --> 02:23.350
looking appearance and unexciting sounding name, it is anything

02:23.420 --> 02:27.002
but an ordinary. And it is very exciting, because inside this

02:27.056 --> 02:30.530
to you is four independent computers.

02:30.630 --> 02:34.522
But for what we're doing. Four nodes,

02:34.666 --> 02:38.094
please. We want eight. Inside each

02:38.132 --> 02:41.562
of these is a completely independent motherboard.

02:41.706 --> 02:45.486
384 gigs of memory, an AMD epic Genoa

02:45.518 --> 02:49.694
processor with 64 cores, dual M two slots

02:49.742 --> 02:53.346
for redundant boot drives. Six pcie gen

02:53.448 --> 02:56.694
five, two and a half inch NVMe slots up front.

02:56.812 --> 02:59.794
And we've got I o in the rear.

02:59.922 --> 03:03.890
Now, this bit here could be a little confusing at first glance,

03:03.970 --> 03:08.166
but that is because not only do we have USB,

03:08.278 --> 03:12.534
but we have two full gen, five x 16 PCIe

03:12.582 --> 03:16.346
connections back here, along with display output and power for

03:16.368 --> 03:20.118
the entire server. This whole thing slides

03:20.214 --> 03:24.106
into the chassis, which holds a really cool modular backplane

03:24.138 --> 03:27.406
assembly that we'll take a look at in a minute. And then passes through.

03:27.508 --> 03:31.226
Thank you, Jake. To the back of the server, where you've

03:31.258 --> 03:34.874
got a management port. A single USB port for each server.

03:34.922 --> 03:38.834
Nope. It's two. And they're shared. What the. I was about

03:38.872 --> 03:42.146
to ask because we've also got a single VGA. You see

03:42.168 --> 03:45.510
the button for two servers there? No way. This button

03:45.580 --> 03:46.840
toggles. Yeah,

03:48.330 --> 03:51.846
and before we talk about that a little bit more,

03:52.028 --> 03:54.866
look at these power supplies.

03:55.058 --> 03:58.818
Each of these is 2200 watts,

03:58.914 --> 04:01.994
80 plus titanium, which sounds like a lot.

04:02.032 --> 04:05.434
But when you're potentially handling 4400

04:05.472 --> 04:09.398
watt epic Genoa CPUs, along with a bunch of rAM, up to 24 NVMe drives

04:09.414 --> 04:13.342
and eight network cards, well, it seems downright reasonable, doesn't it?

04:13.396 --> 04:17.440
Is it 24 drives can't be. Yes. Six times four is 24.

04:18.130 --> 04:21.950
And of course, that's just one of them. We've got

04:22.020 --> 04:25.106
two of those. And that means that in the event that one of

04:25.128 --> 04:28.766
these dies, the system should be able to continue to operate

04:28.878 --> 04:33.182
uninterrupted, which is a big part of the high availability goal

04:33.246 --> 04:36.718
that we have for this deployment. Speaking of high

04:36.744 --> 04:40.502
availability, let's move on to our network cards. Each of

04:40.556 --> 04:43.858
those PCIe gen five X 16 slots I showed

04:43.874 --> 04:47.882
you guys before terminates in one of these OCP 30 small

04:47.936 --> 04:51.242
form factor mezzanine slots. And what we're putting

04:51.296 --> 04:54.966
in them is these connectx 6200 gigabit

04:54.998 --> 04:59.286
cards from Melanox. Excuse me, from Nvidia.

04:59.398 --> 05:02.922
That. Okay, these are the older gen four ones.

05:03.056 --> 05:06.894
So they're going to be limited by the slot speed of around 250

05:06.932 --> 05:10.302
gigabit per second. But if we had newer cards, that means

05:10.356 --> 05:13.886
that each of these nodes could do 200,

05:13.988 --> 05:18.446
plus another 200 400, up to 800 gigabit.

05:18.478 --> 05:21.826
Which would, of course, be a complete waste for us. A, because our

05:21.848 --> 05:26.294
workload can't take advantage of it, and B, because our switch is only

05:26.492 --> 05:29.814
100 gigabit. Sorry. Of course,

05:29.852 --> 05:33.590
the two ports are still helpful. We do have redundant switches,

05:34.250 --> 05:37.766
except there's kind of a problem here. That's still a single point of

05:37.788 --> 05:41.142
failure. In a perfect world, we would have two single port

05:41.196 --> 05:44.826
nicks. So if a nick were to die, it would still be okay.

05:44.928 --> 05:48.794
But because we have so many nodes, we're not really worried about

05:48.832 --> 05:52.406
an individual node. They could have one boot drive and it die,

05:52.438 --> 05:55.950
or one nick and it die. We still have an extra backup.

05:56.290 --> 05:59.806
How many nines do you want? I mean, I don't know. Like one would

05:59.828 --> 06:03.434
be good, 9%. Which, jokes aside,

06:03.482 --> 06:07.410
is a really good point. If we were architecting this properly, there are

06:07.480 --> 06:11.186
so many more considerations that we would need to make. Like the

06:11.208 --> 06:14.526
power coming into the rack would have to come from two independent,

06:14.558 --> 06:18.146
backed up sources. The connectivity to our clients would have

06:18.168 --> 06:21.366
to be redundant as well. The connectivity between all

06:21.388 --> 06:23.974
of the systems would have to be architected in such a way that no matter

06:24.012 --> 06:27.442
what fails, everything will stay up. And realistically

06:27.506 --> 06:31.882
for us, we're not going to get that deep into it because our goal is

06:32.016 --> 06:35.354
better than we had before, which was a single machine with its own

06:35.392 --> 06:38.986
built in redundancies. But other than that, nothing. Now at

06:39.008 --> 06:42.138
least we should be able to lose a full machine out of these eight.

06:42.224 --> 06:45.838
We can restart one of our core switches, totally fine. Two machines out of

06:45.844 --> 06:49.806
these eight and we can still be limping along.

06:49.908 --> 06:53.118
I mean, limping is a bit of a stretch. It's going to be very fast.

06:53.204 --> 06:56.078
Now, normally, if you buy a super micro machine, they're going to prebuild it for

06:56.084 --> 06:59.570
you. They're going to validate it for you. You can even have them pre build

06:59.640 --> 07:03.406
an entire rack or racks of these things and then validate

07:03.438 --> 07:06.658
your application on it before it ships to you. In fact,

07:06.744 --> 07:09.906
we've got a whole video that we did about that. That was sponsored by Supermicro

07:09.938 --> 07:13.394
a little while back. Of course. This is LTT,

07:13.442 --> 07:16.562
my friends. So we will be assembling

07:16.626 --> 07:19.906
this one ourselves. Do you like that spin of the screwdriver above

07:19.938 --> 07:23.194
the server? Don't worry, I won't miss. I'll never miss. See,

07:23.232 --> 07:25.946
I could do this a hundred times and I would never miss. Why? Oh,

07:25.968 --> 07:28.938
you. No, it's fine. It's good. It's okay. We have seven more.

07:29.024 --> 07:32.306
Anywho, for our CPU, we've gone with an epic Genoa

07:32.358 --> 07:35.562
95 34. This is a 64 core,

07:35.626 --> 07:39.322
128 thread monster of a CPU.

07:39.386 --> 07:43.382
It'll do 3.7 GHz max boost. It has a quarter

07:43.466 --> 07:47.086
gigabyte of level three cache, a 300 watt TDP.

07:47.118 --> 07:51.166
It supports DDR five memory up to twelve channels,

07:51.278 --> 07:55.006
and it supports a whopping 128 lanes

07:55.038 --> 07:58.486
of PCIe gen five. Originally, we were

07:58.508 --> 08:02.162
intending to go with 32 core chips, but they were out of stock.

08:02.226 --> 08:05.526
So free upgrade. Lucky us. Compared to

08:05.548 --> 08:08.854
previous generation AMD Epic CPUs, Genoa is a big

08:08.892 --> 08:12.298
step up in terms of I O performance, which makes it

08:12.384 --> 08:16.198
perfect for this application. And in the long term, I mean, if we've

08:16.214 --> 08:19.500
got all the extra CPU cores and a whole bunch of RAM anyway,

08:19.950 --> 08:23.886
why run WECA on the bare metal when we could install Proxmox and

08:23.908 --> 08:28.794
then use the other course for, I don't know, high availability plex

08:28.842 --> 08:32.766
server or Linux ISOs? More realistically, it would

08:32.788 --> 08:35.966
be something like active directory. Yeah. Which we don't really want

08:35.988 --> 08:39.246
to do right now because if you run active directory on one server

08:39.278 --> 08:42.530
and it goes down, you're going to have a really, really bad time.

08:42.600 --> 08:46.434
But if you run it on a bunch of servers, yeah, it's great.

08:46.552 --> 08:50.134
So normally server CPU coolers would come with their own

08:50.172 --> 08:53.894
thermal paste preapplied. But since we're doing this ourselves, and if

08:53.932 --> 08:57.478
you look carefully, it's not the first time that it's been installed, we are

08:57.484 --> 09:01.094
going to be using. Okay, thank you for that. A piece of Honeywell

09:01.142 --> 09:05.690
PTM 79 50. This stuff is freaking awesome.

09:05.840 --> 09:09.066
It has great thermal transfer properties and it can

09:09.088 --> 09:13.130
handle varying temperatures. Seriously, I don't remember.

09:13.280 --> 09:16.890
Not even just varying, but like a lot of huge cycles

09:16.970 --> 09:20.894
for a very, very long time. Now available lttstore.com.

09:20.932 --> 09:24.430
Is that big enough? Does that cover all of the CCDs and CCXs?

09:24.770 --> 09:28.046
Oh, there's a second piece of plastic. Am I stupid? Is there a second piece

09:28.068 --> 09:30.610
of plastic? No, there isn't. Should I go put one in the fridge? No,

09:30.680 --> 09:34.126
it's totally fine. I've done this like a bunch of times now. She's mint.

09:34.158 --> 09:37.766
Look at that. See? Easy. I would recommend putting it in the fridge before you

09:37.788 --> 09:40.998
use it. All right. To ensure we're making the absolute

09:41.084 --> 09:44.626
most of our CPU, especially in this high throughput storage

09:44.658 --> 09:48.158
workload, we're going to be populating all twelve of our memory

09:48.194 --> 09:52.122
channels with 32 gig dims of DDR five ECC running

09:52.176 --> 09:55.738
at 4800 megatransfers per second. That's a total

09:55.824 --> 09:59.354
of 384,

09:59.392 --> 10:02.542
about three terabytes of memory. What?

10:02.676 --> 10:04.800
Across all eight? Oh,

10:06.210 --> 10:09.998
each of the cables Jake's removing right now is a PCIe by eight

10:10.084 --> 10:13.614
cable that feeds two of the drive bays in the front. But the reason

10:13.652 --> 10:18.018
he's taking them out is so that we can install our boot drives. These are

10:18.184 --> 10:21.566
consumer grade. Each system is getting two sabrent

10:21.598 --> 10:25.266
512 gig gen three rocket drives. And it's not

10:25.288 --> 10:28.514
because they're particularly special in any meaningful way.

10:28.552 --> 10:32.166
They're not even that fast by modern standards. But what they are is, from our

10:32.188 --> 10:35.830
experience, reliable enough. And they are fast enough

10:35.900 --> 10:39.446
for what we're going to be doing, which is just booting our operating system off

10:39.468 --> 10:42.954
of them. Movie magic. All of the other nodes are already built.

10:43.152 --> 10:45.994
What do you mean, movie magic? Supermicro built them. Oh, I thought you built them.

10:46.032 --> 10:49.498
Super micro built them for you. I took it apart. Okay, fine.

10:49.584 --> 10:53.254
I took that one apart. No secrets left anymore. Yep. No intrigue.

10:53.302 --> 10:56.494
No mystery. You know, what is still mysterious is inside of here. I've actually never

10:56.532 --> 10:59.070
opened this before. Oh. Okay, let's have a look. Whoo.

10:59.410 --> 11:02.382
Holy power supplies. Yeah. This is so cool.

11:02.436 --> 11:05.806
So the whole computer is cooled by four fans? No way.

11:05.908 --> 11:08.898
There's the two power supply fans, and then these fans in their. What do they

11:08.904 --> 11:11.618
call this? Like, I O module, I think is what they call it. Look at

11:11.624 --> 11:15.138
the blades on this thing counter rotating. You're serious? That's what

11:15.144 --> 11:18.434
you're looking at? Not this. The most delicate of spaghetti.

11:18.482 --> 11:21.398
Oh, my God. There's not even connectors. No.

11:21.484 --> 11:25.014
Every one of these wires is soldered directly to the back

11:25.052 --> 11:27.622
of the OCP 30. What?

11:27.756 --> 11:31.174
Yeah. For storage, we're installing two of Kyoksia's speedy

11:31.222 --> 11:35.606
CD six gen four NVMe drives in each node.

11:35.718 --> 11:39.162
So we've got one that is seven terabytes and another

11:39.216 --> 11:42.886
one that is 15 terabytes. They're kind of placeholders.

11:42.918 --> 11:45.358
For now. And in the long term, we're going to switch to something in the

11:45.364 --> 11:48.766
neighborhood of about 415 terabyte drives per

11:48.788 --> 11:51.838
node. But the drives we want to use are currently occupied by. Oh,

11:51.924 --> 11:55.526
that project? By a top secret pastry

11:55.578 --> 11:59.074
related project. So that's going to have to wait. The good news is that

11:59.112 --> 12:02.846
when those drives become available, WeCA supports live upgrading

12:02.878 --> 12:06.274
and downgrading. So we can just pull these drives, swap in the new ones.

12:06.312 --> 12:09.858
Pull, swap, pull, swap, pull, swap. As long as we don't do it all at

12:09.864 --> 12:13.366
once. Are we ready to fire these things up? Okay, there's a lot going on

12:13.388 --> 12:15.846
here. What is that? Is that a switch? Yeah. Hey, look, you can see the

12:15.868 --> 12:17.560
button now. Oh, that's cool.

12:18.890 --> 12:24.426
What you're hearing so far is just the Nvidia SN 3732

12:24.448 --> 12:28.218
port 200 gig switch. Oh, my God. It even says melanox on

12:28.224 --> 12:31.322
the front. I know. Maybe it's an old, like, review. Sample demo

12:31.376 --> 12:34.686
unit. I got it with the $1 million PC, and I'm pretty sure that that

12:34.708 --> 12:37.850
was already Nvidia at that point. Can you hear that? You hear it? Getting louder.

12:37.930 --> 12:41.486
Yeah. Whoa. That one's just excited to

12:41.508 --> 12:44.778
see. This is the WECA dashboard. Maybe if I go over

12:44.804 --> 12:48.078
here. Cluster servers. We can see all of our servers.

12:48.254 --> 12:51.854
We have two drives per and then cores.

12:51.902 --> 12:55.682
This is a very interesting part of how WECA works. It's not like true

12:55.736 --> 12:59.078
Nas, let's say, where it just uses the whole CPU for whatever you're trying to

12:59.084 --> 13:02.870
do. They dedicate and fence off specific

13:02.940 --> 13:06.934
cores for specific tasks. For instance, each drive gets

13:06.972 --> 13:10.362
a core, so we've got two drive containers. That means two

13:10.416 --> 13:13.900
cores. A full core per drive. Yeah.

13:14.270 --> 13:18.582
Damn. Yeah. You also have compute cores, which do like the parity

13:18.646 --> 13:21.786
calculation, any intercluster communication. And then

13:21.808 --> 13:25.462
there's front end, which you don't necessarily always have. Front end cores manage

13:25.536 --> 13:29.086
connecting to a file system. So if you just had drives and

13:29.108 --> 13:32.234
compute, you wouldn't be able to access the files on this machine.

13:32.282 --> 13:36.154
So you would have your back end servers, right. Those would run drives and compute,

13:36.282 --> 13:40.034
which is the cluster. And then on your GPU box you would

13:40.072 --> 13:43.426
run just the front end, and that would allow the GPU box to connect to

13:43.448 --> 13:46.546
the back end cluster servers. Oh, but I see the back end

13:46.568 --> 13:49.890
cluster servers don't need to run a front end unless

13:50.050 --> 13:53.798
you want to be able to access the files on that machine or

13:53.964 --> 13:57.858
from that machine, which we want to. Right. Because we're using SMB.

13:57.954 --> 14:01.702
We're using it as a file server. Stupid NAs for our stupid

14:01.766 --> 14:05.066
windows machines. Yeah. You can also have a

14:05.088 --> 14:08.870
dedicated front end machine. Yes. So if you had like 100 backend,

14:08.950 --> 14:11.866
but then that's adding a single point of failure, which is what we're trying to

14:11.888 --> 14:15.374
avoid. You could have multiple of them. Okay, they thought of that?

14:15.412 --> 14:18.926
Yeah, I set it up. So every single machine in the

14:18.948 --> 14:22.330
cluster, all eight of them, are part of our SMB cluster,

14:22.410 --> 14:25.182
which means it cannot go down. Yeah.

14:25.236 --> 14:28.658
Realistically, there are a ton of other file systems out there that you

14:28.664 --> 14:32.386
could use for something like this. Truenas has their scale out setup for

14:32.408 --> 14:36.146
clustered ZFs, which only requires three nodes and is

14:36.168 --> 14:39.286
something we'd be quite interested in trying out. Or if you're

14:39.308 --> 14:42.982
looking for object storage, there's a million options, but the main open

14:43.036 --> 14:46.774
source one, minio, requires only four nodes though.

14:46.812 --> 14:50.006
When we saw how nuts WECA was, when we set up the million dollar server

14:50.038 --> 14:54.090
cluster, I mean, we had to try it out for ourselves

14:54.430 --> 14:57.706
and try it out. We did. So this

14:57.728 --> 15:02.842
is each node. Holy fucking look.

15:02.896 --> 15:06.366
Okay, the crazy thing is, look at the read latency now, guys, look, hold on,

15:06.388 --> 15:10.366
hold on, hold on. At 70GB a second. We've seen

15:10.468 --> 15:14.538
numbers like this before, but we're talking with, in some cases,

15:14.634 --> 15:18.754
double the number of drives and no file without a file system like

15:18.792 --> 15:22.242
raw to each drive. This is with a file system,

15:22.296 --> 15:25.726
with a file system over a network. And we're

15:25.758 --> 15:29.666
only using 100 gig ports. Like usually with a WEcA

15:29.698 --> 15:33.574
setup like this, you'd probably use 200. Yeah. Oh my

15:33.612 --> 15:37.266
God. We didn't know because we didn't even have networking

15:37.378 --> 15:40.634
as a factor. Last time, all the drives were in one box.

15:40.672 --> 15:43.866
I know. This is networking too. And the crazy part is, we're not

15:43.888 --> 15:46.838
using RDMA. This is like some fancy.

15:47.014 --> 15:49.986
What's it called? DPDK, I think is the library.

15:50.118 --> 15:53.898
This is wild. Yeah, look at that. Read latency,

15:53.994 --> 15:58.190
131 microseconds. That's 4 million read

15:58.260 --> 16:01.946
I ops with a latency of one millisecond average.

16:02.058 --> 16:05.674
Are we able to keep using WecFs? This is a trial.

16:05.802 --> 16:09.810
Okay. This software is quite expensive. This is unreal. 4 million iOS.

16:10.470 --> 16:14.660
It is unreal. It's way more than we could possibly ever need.

16:15.030 --> 16:18.686
But it's cool. So cool. Don't they support tiering

16:18.718 --> 16:21.638
and everything? Oh, yeah. Here, I'll show you, actually, what that looks like. This is

16:21.644 --> 16:25.314
on mother vault, which I think right now has 400 tippy bytes

16:25.362 --> 16:28.594
left. So let's say max capacity is 400 terabytes.

16:28.642 --> 16:31.898
Now, once we run out of the 100 terabytes of

16:31.904 --> 16:35.606
SSD capacity, which you can see here, it'll tear.

16:35.638 --> 16:38.826
I mean, it automatically tears anyways. And you do need to make sure

16:38.848 --> 16:41.998
that your object store is at least the same size as the

16:42.004 --> 16:45.630
flash or bigger because they're going to automatically tier everything

16:45.700 --> 16:50.298
to it. That makes sense. So in theory, we move manually

16:50.394 --> 16:54.410
copy everything from vault one time to weco

16:54.490 --> 16:58.430
one time because it stores in like 64 megabyte chunks.

16:58.510 --> 17:01.634
And then it just stays there forever. Stays there forever. And then we just have

17:01.672 --> 17:05.330
one network share. And when something needs to get vaulted,

17:05.670 --> 17:08.786
you just move it from. Allow it to decay. Yeah. You would probably move it

17:08.808 --> 17:12.294
from pending projects to like, done or something like that. We make a folder for

17:12.332 --> 17:16.326
done? Yeah, sure. And then it will just do it automatically. Wow. Or if

17:16.348 --> 17:19.926
it's a video that somebody was working on and

17:19.948 --> 17:23.926
then it's been on hold for three months and we shot footage,

17:23.958 --> 17:26.618
it will just. And then when we're ready to work on it, it'll promote it

17:26.624 --> 17:30.506
back up. Holy. We could net boot off of this follow up video.

17:30.608 --> 17:33.854
Yeah. I mean, why not? It's so fast. Literally could

17:33.892 --> 17:37.502
not. We couldn't saturate this. Now, a lot of you at this point

17:37.556 --> 17:41.786
must be thinking, gosh, Mr. That's an awful lot of computers

17:41.818 --> 17:45.780
for high availability. Couldn't you do this with two?

17:46.150 --> 17:50.514
And you're not that far off the old school high

17:50.552 --> 17:54.414
availability NetApp storage appliances like that one we looked at recently

17:54.542 --> 17:57.106
did have just two machines,

17:57.298 --> 18:00.550
but those were both connected to the

18:00.620 --> 18:04.342
same storage drives. If each system has its

18:04.396 --> 18:07.926
own drives, when things can get out of sync, like, let's say

18:07.948 --> 18:12.090
if one machine has downtime, you can run into a situation where each

18:12.160 --> 18:15.674
system believes with all the conviction in its heart

18:15.792 --> 18:19.306
that it has the correct data. And then if

18:19.328 --> 18:23.454
all you have is two, how will they decide who's right?

18:23.652 --> 18:27.466
This is typically referred to as split brain, and that's

18:27.498 --> 18:31.102
why the majority of high availability systems have,

18:31.156 --> 18:34.142
at bare minimum, three servers.

18:34.286 --> 18:38.098
This allows the third system to be a tiebreaker of sorts in

18:38.104 --> 18:41.490
the case of a disagreement. Now, in our case,

18:41.640 --> 18:45.070
WECA, that stupid ultra fast file system that

18:45.080 --> 18:48.806
we're using, which unlike anything that we've used before, has been

18:48.828 --> 18:52.466
built specifically for NVMe drives, not hard drives.

18:52.578 --> 18:56.290
Well, it requires a minimum of six nodes

18:56.370 --> 19:00.290
with a recommendation of eight, but running WECA can still

19:00.300 --> 19:03.334
be an advantage. Video editing with Adobe premiere,

19:03.382 --> 19:06.954
like we use, is very latency sensitive, and even

19:06.992 --> 19:10.266
a small delay when going to access a clip can be enough to make the

19:10.288 --> 19:14.142
software crash. So any improvement there is huge.

19:14.276 --> 19:17.546
Not to mention that a pair of these grand twins

19:17.658 --> 19:20.746
specked out to the max with 128 core epic

19:20.778 --> 19:24.766
burgermo CPUs would get you just four rack

19:24.798 --> 19:28.702
units with 1000 CPU cores.

19:28.766 --> 19:31.986
Actually, a little more. 24 terabytes of DDR five,

19:32.088 --> 19:35.306
and up to three petabytes of NVMe storage.

19:35.358 --> 19:38.950
I mean, that makes our setup seem downright reasonable.

19:39.770 --> 19:43.254
Now, the average WeCA customers are going to be a little more

19:43.292 --> 19:47.106
demanding than us. Visual effects studios, AI developers,

19:47.218 --> 19:51.094
genomics labs, all the folks out there that need stupid

19:51.142 --> 19:55.018
fast, low latency storage. And WECA showed us screenshots of clusters that

19:55.024 --> 19:58.934
were reading in excess of 1 tb/second consistently.

19:59.062 --> 20:03.066
Obviously that was a bigger cluster, but it shows you what can be achieved

20:03.178 --> 20:06.334
with this kind of hardware running on. I mean,

20:06.372 --> 20:08.974
what used to be the crappier option?

20:09.172 --> 20:12.414
Software raid. Man, I feel bad

20:12.452 --> 20:16.260
even calling it that these days. I had an interesting

20:16.630 --> 20:19.826
idea with the super micro folks. So you know how we

20:19.848 --> 20:23.534
have like two petabytes of 13 years worth of footage?

20:23.662 --> 20:27.106
Thousands and thousands of hours of footage? Thousands.

20:27.218 --> 20:30.386
It's really cool that we have it, but it's

20:30.418 --> 20:34.134
really hard to use unless you just happen to know what

20:34.172 --> 20:37.238
video the thing you were looking for is in.

20:37.324 --> 20:40.674
Well, what if you could just search for something?

20:40.812 --> 20:44.298
Linus Sebastian. I want every clip with Linus Sebastian in it. Wow. Bam. Look at

20:44.304 --> 20:47.898
that. Shut up and let's know. There's this one

20:47.984 --> 20:50.982
that's detected that it's you throughout the entire clip.

20:51.046 --> 20:54.442
Yeah. You're in a chair. So you could search for clips

20:54.586 --> 20:58.078
of Linus sitting down with a

20:58.084 --> 21:01.630
keyboard. Yeah. Like we're going to be able to actually find stuff.

21:01.780 --> 21:05.194
Yeah. Right now there is a finite amount of objects

21:05.242 --> 21:08.298
that are trained. I mean, Chihuahua. Let me scroll through this.

21:08.324 --> 21:11.970
It's a lot. Eventually you'll be able to train it and tell it, hey,

21:12.040 --> 21:14.754
this is what a computer fan looks like. Or this is what an SSD looks

21:14.792 --> 21:18.614
like. Oh, my God, that is so cool. So wait, is this running

21:18.732 --> 21:22.358
on these extra CPU cores? Okay, no, not right now.

21:22.444 --> 21:26.114
Faces and logos are running on CPU. Yeah. Objects,

21:26.162 --> 21:29.574
OCR and scenes run on GPU. Got it. But they're not running

21:29.612 --> 21:32.746
on any of those machines. They're running on a GPU workstation that's super

21:32.768 --> 21:35.910
microsent that's sitting at my desk. It was heavy. Anyways,

21:35.990 --> 21:38.966
what is happening on that new server is proxies.

21:39.158 --> 21:42.410
Because if we were to analyze the original clips,

21:43.570 --> 21:47.118
formatting is a huge problem when you go into an AI model. It might

21:47.124 --> 21:50.782
not necessarily support the codec that you're filming in. Sure. But also

21:50.916 --> 21:54.470
clips are like hundreds of megabytes a second. Potentially,

21:54.570 --> 21:58.146
that would take forever. So instead it generates proxies of

21:58.168 --> 22:01.422
everything first, which we're dumping to that new server.

22:01.566 --> 22:05.070
And then we can take advantage of the lightning fast storage.

22:05.150 --> 22:08.506
Yeah, you can see we have 2.6 terabytes massive

22:08.558 --> 22:12.770
compute. And we can basically create like a proxy

22:12.930 --> 22:16.630
map of what everything is in the main archive. Right.

22:16.700 --> 22:20.430
That is so cool. So far, I've generated 2.6 terabytes

22:20.450 --> 22:23.994
of proxies, which might not sound like a lot, but they're only five

22:24.032 --> 22:27.466
megabits, so it's actually like a lot. This is

22:27.488 --> 22:31.130
going to be a flipping game changer. News, sports,

22:31.870 --> 22:35.422
can you imagine your CNN? You want that person

22:35.476 --> 22:39.262
wearing a red tie. Yeah, but right now we've done 25,000.

22:39.316 --> 22:42.606
So 2.6 terabytes is 25,000 proxy. Okay, well, let's try

22:42.628 --> 22:46.286
and find something. Oh, hold on. Once you've generated a proxy,

22:46.398 --> 22:50.494
you have to then analyze it, right? So the analysis

22:50.542 --> 22:54.030
is not done? No, not even close. I've analyzed 22 clips.

22:54.110 --> 22:56.770
Okay? Everything was Elijah. Elijah.

22:57.670 --> 23:01.046
And this is the every clip that Elijah's in. And you can even see.

23:01.148 --> 23:04.582
So cool. This is the actual mam, as they call it,

23:04.636 --> 23:07.974
media asset manager. Yeah, the actual AI guys built

23:08.012 --> 23:11.386
this before it was like AI, as far as I'm aware. Back when you would

23:11.408 --> 23:15.222
have had to make comments like this manually. Now it's just AI.

23:15.366 --> 23:19.302
So all of the data is in here now. And we can see here's Adam

23:19.366 --> 23:22.554
and Elijah. Oh, that's so cool. Here's all the different

23:22.592 --> 23:24.766
objects. Chair, flower pot,

23:24.868 --> 23:28.286
microphone. Oh, let me show you the scene understanding thing,

23:28.308 --> 23:31.326
because that is so cool. This is like a brand new thing. They barely even

23:31.348 --> 23:34.942
worked it in, but it basically takes

23:34.996 --> 23:38.626
a snapshot every second. Two men are working on

23:38.648 --> 23:42.302
a project in a room. There is a speaker, stereo equipment.

23:42.366 --> 23:45.458
There's a faucet, there's a tripod. There's the tripod. Some of

23:45.464 --> 23:48.974
these are a little less accurate. Two men are working on a robot

23:49.022 --> 23:52.182
in a room. It kind of looks like a robot. I mean, yeah,

23:52.236 --> 23:55.206
sure. Two men are in a workshop looking at a laptop computer,

23:55.308 --> 23:58.822
looking at a machine. There is person Alex Clark. So this is just

23:58.876 --> 24:02.038
running right now in real time. Like, more stuff is getting processed. Hey, see right

24:02.044 --> 24:05.162
here? Processing logos. There it is.

24:05.216 --> 24:08.618
Processing logos and faces. It's going to take a while. Yeah, it's going to

24:08.624 --> 24:12.394
take forever. They're still working on making it function on multiple

24:12.442 --> 24:15.886
GPUs. So once we can get it running on,

24:15.908 --> 24:19.518
like, four GPUs, say one GPU is doing face detection, one's doing

24:19.684 --> 24:22.910
scene analysis, one's doing object detection or something like that,

24:22.980 --> 24:25.930
we'll be able to go a lot faster. But right now, it's just one GPU.

24:26.010 --> 24:29.294
Got it. But this is so cool. All that's left is to deploy

24:29.342 --> 24:32.306
it. Linus had to run away to do some other stuff, so I've hired some

24:32.328 --> 24:35.406
backup cavalry, Sean, our infrastructure administrator.

24:35.518 --> 24:39.046
Except we've run into a bit of a problem. Linus and me, in our

24:39.068 --> 24:42.134
infinite wisdom, while we were making this rack so much

24:42.172 --> 24:45.446
better, ran a bunch of cables right where we need to put the

24:45.468 --> 24:48.598
server. Did we just start unplugging? No. Yeah.

24:48.684 --> 24:52.362
How are we even gonna do this? We have to part the seas. I started

24:52.416 --> 24:55.002
to try to move some of the cables out of the way, but they're all

24:55.056 --> 24:58.602
twisted together. So hopefully the LTP cable management thing,

24:58.656 --> 25:02.382
which you can finally get@ltpstore.com. Will save us.

25:02.516 --> 25:06.158
Beautiful. Cable manage. We can slide a server in there now,

25:06.324 --> 25:09.680
I hope. Yeah, it's on.

25:11.330 --> 25:14.914
Okay, you're good. Just go. That wasn't so bad.

25:14.952 --> 25:16.820
Like it was made for it. Next.

25:19.190 --> 25:22.850
Hey, we're in. Now we just have to run a million

25:22.920 --> 25:26.302
cables. Do you notice anything different? Well, it's loud.

25:26.366 --> 25:29.358
Most of that's actually just the vent is on. One of the air conditioners is

25:29.384 --> 25:32.886
broken again. But do you notice anything different? I mean, the sticker is

25:32.908 --> 25:36.214
here. That sticker has been there for years. Seriously? You haven't noticed anything else?

25:36.252 --> 25:39.462
Well, you guys screwed something onto the. Oh, did you put sauna pan

25:39.516 --> 25:42.838
behind it? Yeah, but I thought this was supposed to be a vented door.

25:42.934 --> 25:46.762
My original plan was to get rid of the vent that you put in.

25:46.816 --> 25:50.326
But that vent was there as a backup in case HVAC

25:50.358 --> 25:53.606
ever failed. Still works. So that fan is the exhaust, and that's the intake.

25:53.638 --> 25:57.134
You see all the gaps. Oh, my God. But do you notice the sound difference?

25:57.252 --> 26:01.118
Yeah, that's a big difference. It's huge. But that server is so loud, we basically

26:01.204 --> 26:02.640
ended up where we started.

26:04.130 --> 26:07.038
Yeah, but that's okay. I was just trying to normalize. I just mean I didn't

26:07.054 --> 26:09.860
make it worse. It's not that.

26:10.310 --> 26:13.410
Okay. Look at that. Whoo. Cute, right?

26:13.560 --> 26:16.982
God, that's a lot of metal. If all goes to plan,

26:17.116 --> 26:20.774
we could get rid of this and this. And just have these. So no

26:20.812 --> 26:24.120
more additional racks taken up, which is nice. Wow.

26:28.090 --> 26:31.530
It should sustain two entire servers dropping out

26:31.600 --> 26:35.594
without anyone even noticing. Do you really want to test it right now?

26:35.712 --> 26:39.020
I haven't tried that. All right, here we go. What could go wrong?

26:40.190 --> 26:43.582
I mean, a lot. The fact that all the fans just,

26:43.636 --> 26:47.530
like, turned down a bit is a little scary. Let's go see if anyone noticed.

26:47.610 --> 26:50.910
Oh, hi, Mark. Hi. I'm holding your file server. How's your edit?

26:51.570 --> 26:54.682
Huh? Is it working? It's working. Is this on Wi Fi?

26:54.746 --> 26:57.742
Hey, Emily. How's your edit going? I'm holding your server.

26:57.806 --> 27:01.058
That's cool. Is it working? Are you sure? Yeah.

27:01.144 --> 27:04.658
Hoffman, how's your edit going? This is your server right here. Amazing.

27:04.744 --> 27:08.066
Look, feel it. It's still warm. Wow. Yeah, it's still warm. Well, how's it

27:08.088 --> 27:10.742
working? It's great. You know, I'm editing the video that we're shooting. You are?

27:10.796 --> 27:14.086
Yeah. We're going to pull another one. Wait, no, Linus, you forgot one. Yeah,

27:14.108 --> 27:17.834
here's another. Here's another one of your servers. Is it working? It's great.

27:17.872 --> 27:21.242
Though, for reference, you're not supposed

27:21.296 --> 27:24.362
to do this. You should power off the system first.

27:24.416 --> 27:28.342
But we're just trying to simulate it. Failing. Yeah. A terrible,

27:28.486 --> 27:32.054
catastrophic failure. I can't believe how smoothly

27:32.102 --> 27:36.346
it handled that. See all the lights? They never stopped blinking big. Thanks to Supermicro

27:36.378 --> 27:40.202
for these awesome servers. Thanks to Weca for making this crazy software. Thanks to Axel

27:40.266 --> 27:43.118
AI for the awesome AI detection. If you like this video,

27:43.204 --> 27:46.698
maybe check out the video series of us building our nearly three petabytes

27:46.714 --> 27:50.462
of archival storage, which we call the mother vault. That thing is awesome

27:50.596 --> 27:54.078
and we showed it to you and it's faster now. Oh, and thanks to you

27:54.164 --> 27:55.490
for being an awesome some viewers.
